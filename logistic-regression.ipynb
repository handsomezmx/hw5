{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T19:59:43.120142Z",
     "start_time": "2020-02-03T19:59:43.109299Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T19:59:43.655832Z",
     "start_time": "2020-02-03T19:59:43.642041Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 1)\n",
      "[[ 5.8  4.   1.2  0.2 -1. ]\n",
      " [ 5.1  2.5  3.   1.1 -1. ]\n",
      " [ 6.6  3.   4.4  1.4 -1. ]\n",
      " [ 5.4  3.9  1.3  0.4 -1. ]\n",
      " [ 7.9  3.8  6.4  2.   1. ]\n",
      " [ 6.3  3.3  4.7  1.6 -1. ]\n",
      " [ 6.9  3.1  5.1  2.3  1. ]\n",
      " [ 5.1  3.8  1.9  0.4 -1. ]\n",
      " [ 4.7  3.2  1.6  0.2 -1. ]\n",
      " [ 6.9  3.2  5.7  2.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3 -1. ]\n",
      " [ 5.4  3.9  1.7  0.4 -1. ]\n",
      " [ 7.1  3.   5.9  2.1  1. ]\n",
      " [ 6.4  3.2  4.5  1.5 -1. ]\n",
      " [ 6.   2.9  4.5  1.5 -1. ]\n",
      " [ 4.4  3.2  1.3  0.2 -1. ]\n",
      " [ 5.8  2.6  4.   1.2 -1. ]\n",
      " [ 5.6  3.   4.5  1.5 -1. ]\n",
      " [ 5.4  3.4  1.5  0.4 -1. ]\n",
      " [ 5.   3.2  1.2  0.2 -1. ]\n",
      " [ 5.5  2.6  4.4  1.2 -1. ]\n",
      " [ 5.4  3.   4.5  1.5 -1. ]\n",
      " [ 6.7  3.   5.   1.7 -1. ]\n",
      " [ 5.   3.5  1.3  0.3 -1. ]\n",
      " [ 7.2  3.2  6.   1.8  1. ]\n",
      " [ 5.7  2.8  4.1  1.3 -1. ]\n",
      " [ 5.5  4.2  1.4  0.2 -1. ]\n",
      " [ 5.1  3.8  1.5  0.3 -1. ]\n",
      " [ 6.1  2.8  4.7  1.2 -1. ]\n",
      " [ 6.3  2.5  5.   1.9  1. ]\n",
      " [ 6.1  3.   4.6  1.4 -1. ]\n",
      " [ 7.7  3.   6.1  2.3  1. ]\n",
      " [ 5.6  2.5  3.9  1.1 -1. ]\n",
      " [ 6.4  2.8  5.6  2.1  1. ]\n",
      " [ 5.8  2.8  5.1  2.4  1. ]\n",
      " [ 5.3  3.7  1.5  0.2 -1. ]\n",
      " [ 5.5  2.3  4.   1.3 -1. ]\n",
      " [ 5.2  3.4  1.4  0.2 -1. ]\n",
      " [ 6.5  2.8  4.6  1.5 -1. ]\n",
      " [ 6.7  2.5  5.8  1.8  1. ]\n",
      " [ 6.8  3.   5.5  2.1  1. ]\n",
      " [ 5.1  3.5  1.4  0.3 -1. ]\n",
      " [ 6.   2.2  5.   1.5  1. ]\n",
      " [ 6.3  2.9  5.6  1.8  1. ]\n",
      " [ 6.6  2.9  4.6  1.3 -1. ]\n",
      " [ 7.7  2.6  6.9  2.3  1. ]\n",
      " [ 5.7  3.8  1.7  0.3 -1. ]\n",
      " [ 5.   3.6  1.4  0.2 -1. ]\n",
      " [ 4.8  3.   1.4  0.3 -1. ]\n",
      " [ 5.2  2.7  3.9  1.4 -1. ]\n",
      " [ 5.1  3.4  1.5  0.2 -1. ]\n",
      " [ 5.5  3.5  1.3  0.2 -1. ]\n",
      " [ 7.7  3.8  6.7  2.2  1. ]\n",
      " [ 6.9  3.1  5.4  2.1  1. ]\n",
      " [ 7.3  2.9  6.3  1.8  1. ]\n",
      " [ 6.4  2.8  5.6  2.2  1. ]\n",
      " [ 6.2  2.8  4.8  1.8  1. ]\n",
      " [ 6.   3.4  4.5  1.6 -1. ]\n",
      " [ 7.7  2.8  6.7  2.   1. ]\n",
      " [ 5.7  3.   4.2  1.2 -1. ]\n",
      " [ 4.8  3.4  1.6  0.2 -1. ]\n",
      " [ 5.7  2.5  5.   2.   1. ]\n",
      " [ 6.3  2.7  4.9  1.8  1. ]\n",
      " [ 4.8  3.   1.4  0.1 -1. ]\n",
      " [ 4.7  3.2  1.3  0.2 -1. ]\n",
      " [ 6.5  3.   5.8  2.2  1. ]\n",
      " [ 4.6  3.4  1.4  0.3 -1. ]\n",
      " [ 6.1  3.   4.9  1.8  1. ]\n",
      " [ 6.5  3.2  5.1  2.   1. ]\n",
      " [ 6.7  3.1  4.4  1.4 -1. ]\n",
      " [ 5.7  2.8  4.5  1.3 -1. ]\n",
      " [ 6.7  3.3  5.7  2.5  1. ]\n",
      " [ 6.   3.   4.8  1.8  1. ]\n",
      " [ 5.1  3.8  1.6  0.2 -1. ]\n",
      " [ 6.   2.2  4.   1.  -1. ]\n",
      " [ 6.4  2.9  4.3  1.3 -1. ]\n",
      " [ 6.5  3.   5.5  1.8  1. ]\n",
      " [ 5.   2.3  3.3  1.  -1. ]\n",
      " [ 6.3  3.3  6.   2.5  1. ]\n",
      " [ 5.5  2.5  4.   1.3 -1. ]\n",
      " [ 5.4  3.7  1.5  0.2 -1. ]\n",
      " [ 4.9  3.1  1.5  0.2 -1. ]\n",
      " [ 5.2  4.1  1.5  0.1 -1. ]\n",
      " [ 6.7  3.3  5.7  2.1  1. ]\n",
      " [ 4.4  3.   1.3  0.2 -1. ]\n",
      " [ 6.   2.7  5.1  1.6 -1. ]\n",
      " [ 6.4  2.7  5.3  1.9  1. ]\n",
      " [ 5.9  3.   5.1  1.8  1. ]\n",
      " [ 5.2  3.5  1.5  0.2 -1. ]\n",
      " [ 5.1  3.3  1.7  0.5 -1. ]\n",
      " [ 5.8  2.7  4.1  1.  -1. ]\n",
      " [ 4.9  3.1  1.5  0.1 -1. ]\n",
      " [ 7.4  2.8  6.1  1.9  1. ]\n",
      " [ 6.2  2.9  4.3  1.3 -1. ]\n",
      " [ 7.6  3.   6.6  2.1  1. ]\n",
      " [ 6.7  3.   5.2  2.3  1. ]\n",
      " [ 6.3  2.3  4.4  1.3 -1. ]\n",
      " [ 6.2  3.4  5.4  2.3  1. ]\n",
      " [ 7.2  3.6  6.1  2.5  1. ]\n",
      " [ 5.6  2.9  3.6  1.3 -1. ]\n",
      " [ 5.7  4.4  1.5  0.4 -1. ]\n",
      " [ 5.8  2.7  3.9  1.2 -1. ]\n",
      " [ 4.5  2.3  1.3  0.3 -1. ]\n",
      " [ 5.5  2.4  3.8  1.1 -1. ]\n",
      " [ 6.9  3.1  4.9  1.5 -1. ]\n",
      " [ 5.   3.4  1.6  0.4 -1. ]\n",
      " [ 6.8  2.8  4.8  1.4 -1. ]\n",
      " [ 5.   3.5  1.6  0.6 -1. ]\n",
      " [ 4.8  3.4  1.9  0.2 -1. ]\n",
      " [ 6.3  3.4  5.6  2.4  1. ]\n",
      " [ 5.6  2.8  4.9  2.   1. ]\n",
      " [ 6.8  3.2  5.9  2.3  1. ]\n",
      " [ 5.   3.3  1.4  0.2 -1. ]\n",
      " [ 5.1  3.7  1.5  0.4 -1. ]\n",
      " [ 5.9  3.2  4.8  1.8 -1. ]\n",
      " [ 4.6  3.1  1.5  0.2 -1. ]\n",
      " [ 5.8  2.7  5.1  1.9  1. ]\n",
      " [ 4.8  3.1  1.6  0.2 -1. ]\n",
      " [ 6.5  3.   5.2  2.   1. ]\n",
      " [ 4.9  2.5  4.5  1.7  1. ]\n",
      " [ 4.6  3.2  1.4  0.2 -1. ]\n",
      " [ 6.4  3.2  5.3  2.3  1. ]\n",
      " [ 4.3  3.   1.1  0.1 -1. ]\n",
      " [ 5.6  3.   4.1  1.3 -1. ]\n",
      " [ 4.4  2.9  1.4  0.2 -1. ]\n",
      " [ 5.5  2.4  3.7  1.  -1. ]\n",
      " [ 5.   2.   3.5  1.  -1. ]\n",
      " [ 5.1  3.5  1.4  0.2 -1. ]\n",
      " [ 4.9  3.   1.4  0.2 -1. ]\n",
      " [ 4.9  2.4  3.3  1.  -1. ]\n",
      " [ 4.6  3.6  1.   0.2 -1. ]\n",
      " [ 5.9  3.   4.2  1.5 -1. ]\n",
      " [ 6.1  2.9  4.7  1.4 -1. ]\n",
      " [ 5.   3.4  1.5  0.2 -1. ]\n",
      " [ 6.7  3.1  4.7  1.5 -1. ]\n",
      " [ 5.7  2.9  4.2  1.3 -1. ]\n",
      " [ 6.2  2.2  4.5  1.5 -1. ]\n",
      " [ 7.   3.2  4.7  1.4 -1. ]\n",
      " [ 5.8  2.7  5.1  1.9  1. ]\n",
      " [ 5.4  3.4  1.7  0.2 -1. ]\n",
      " [ 5.   3.   1.6  0.2 -1. ]\n",
      " [ 6.1  2.6  5.6  1.4  1. ]\n",
      " [ 6.1  2.8  4.   1.3 -1. ]\n",
      " [ 7.2  3.   5.8  1.6  1. ]\n",
      " [ 5.7  2.6  3.5  1.  -1. ]\n",
      " [ 6.3  2.8  5.1  1.5  1. ]\n",
      " [ 6.4  3.1  5.5  1.8  1. ]\n",
      " [ 6.3  2.5  4.9  1.5 -1. ]\n",
      " [ 6.7  3.1  5.6  2.4  1. ]\n",
      " [ 4.9  3.6  1.4  0.1 -1. ]]\n"
     ]
    }
   ],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "# Here for convenience, we divide the 3 kinds of flowers into 2 groups: \n",
    "#     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "#     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "# Thus we use (iris.target > 1.5) to divide the targets into 2 groups. \n",
    "# This line of code will assign:\n",
    "#    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "#    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "Y = (iris.target > 1.5).reshape(-1,1).astype(np.float) # The shape of Y is (150, 1), which means \n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value. \n",
    "Y[Y==0] = -1\n",
    "\n",
    "X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_and_Y)               # The result should be always: [ 5.8  4.   1.2  0.2  -1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T19:59:44.191113Z",
     "start_time": "2020-02-03T19:59:44.184510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:4]\n",
    "Y_shuffled = X_and_Y[:,4]\n",
    "\n",
    "\n",
    "X_train = X_shuffled[:100][:,[3,1]] # Shape: (100,2)\n",
    "X_train = np.delete(X_train, 42, axis=0) # Remove a point for separability.\n",
    "Y_train = Y_shuffled[:100]          # Shape: (100,)\n",
    "Y_train = np.delete(Y_train, 42, axis=0) # Remove a point for separability.\n",
    "X_test = X_shuffled[100:][:,[3,1]]  # Shape: (50,2)\n",
    "Y_test = Y_shuffled[100:]           # Shape: (50,)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:41:09.008321Z",
     "start_time": "2020-02-03T21:41:08.992524Z"
    }
   },
   "outputs": [],
   "source": [
    "def vis(X, Y, W=None, b=None):\n",
    "    indices_neg1 = (Y == -1).nonzero()[0]\n",
    "    indices_pos1 = (Y == 1).nonzero()[0]\n",
    "    plt.scatter(X[:,0][indices_neg1], X[:,1][indices_neg1], \n",
    "                c='blue', label='class -1')\n",
    "    plt.scatter(X[:,0][indices_pos1], X[:,1][indices_pos1], \n",
    "                c='red', label='class 1')\n",
    "    plt.legend()\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "    \n",
    "    if W is not None:\n",
    "        # w0x0+w1x1+b=0 => x1=-w0x0/w1-b/w1\n",
    "        w0 = W[0]\n",
    "        w1 = W[1]\n",
    "        temp = -w1*np.array([X[:,1].min(), X[:,1].max()])/w0-b/w0\n",
    "        x0_min = max(temp.min(), X[:,0].min())\n",
    "        x0_max = min(temp.max(), X[:,1].max())\n",
    "        x0 = np.linspace(x0_min,x0_max,100)\n",
    "        x1 = -w0*x0/w1-b/w1\n",
    "        plt.plot(x0,x1,color='black')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:41:09.786579Z",
     "start_time": "2020-02-03T21:41:09.572107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training set.\n",
    "vis(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we would like to use the gradient descent to calculate the parameters $\\mathbf{w},b$ for a logistic regression model.\n",
    "If we have the loss function $\\mathcal{L}(\\mathbf{w},b)$, then a typical gradient descent algorithm contains the following steps:\n",
    "\n",
    "**Step 1**. Initialize the parameters $\\mathbf{w}$, $b$.\n",
    "\n",
    "for i = 1 to #iterations:\n",
    "\n",
    "- **Step 2**. Compute the partial derivatives $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}}$, $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$.\n",
    "\n",
    "- **Step 3**. Update the parameters \n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial \\mathbf{w}}, \\quad\\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Note that in the code, we use `W` and `b` to represent the weight vector $\\mathbf{w}$ and bias scalar $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T21:41:10.655445Z",
     "start_time": "2020-02-03T21:41:10.652350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid function: sigmoid(z) = 1/(1 + e^(-z))\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T22:09:32.913380Z",
     "start_time": "2020-02-03T22:09:32.906539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Judge function: 1(a != b).\n",
    "def judge(a, b):\n",
    "    ####### To be filled #######\n",
    "    \n",
    "# Logistic regression classifier.\n",
    "def f_logistic(x, W, b):\n",
    "    # x should be a 2-dimensional vector, \n",
    "    # W should be a 2-dimensional vector,\n",
    "    # b should be a scalar.\n",
    "    # you should return a scalar which is -1 or 1.\n",
    "    ####### To be filled #######  Hint: You may use sigmoid() function.\n",
    "    \n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, W, b):\n",
    "    ############ To be filled. ############\n",
    "    \n",
    "    for (xi, yi) in zip(X, Y):\n",
    "        ############ To be filled. ############\n",
    "        # Hint: Use judge() and f_logistic()\n",
    "    \n",
    "    ############ To be filled. ############\n",
    "    return ############ To be filled. ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T22:24:25.962804Z",
     "start_time": "2020-02-03T22:24:25.956116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W, b) with respect to W and b.\n",
    "def grad_L_W_b(X, Y, W, b):\n",
    "    ####### To be filled #######\n",
    "\n",
    "    grad_W = ####### To be filled #######\n",
    "    grad_b = ####### To be filled #######\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T22:24:26.222737Z",
     "start_time": "2020-02-03T22:24:26.216445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss L(W, b).\n",
    "def L_W_b(X, Y, W, b):\n",
    "    # You should return a scalar.\n",
    "    \n",
    "    ####### To be filled #######\n",
    "    return ####### To be filled #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T23:15:21.854647Z",
     "start_time": "2020-02-03T23:15:21.804978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some settings.\n",
    "learning_rate = 0.001\n",
    "iterations    = 10000\n",
    "losses = []\n",
    "\n",
    "# Gradient descent algorithm for logistic regression.\n",
    "# Step 1. Initialize the parameters W, b.\n",
    "W = np.zeros(2) \n",
    "b = 0\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Step 2. Compute the partial derivatives.\n",
    "    grad_W, grad_b = grad_L_W_b(X_train, Y_train, W, b)\n",
    "    # Step 3. Update the parameters.\n",
    "    ####### To be filled #######   # Update W.\n",
    "    ####### To be filled #######   # Update b.\n",
    "\n",
    "    # Track the training losses.\n",
    "    losses.append(L_W_b(X_train, Y_train, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T22:24:27.534420Z",
     "start_time": "2020-02-03T22:24:27.196471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show decision boundary, training error and test error.\n",
    "print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(W[0],W[1],b))\n",
    "vis(X_train, Y_train, W, b)\n",
    "print('Training error: {}'.format(calc_error(X_train, Y_train, W, b)))\n",
    "vis(X_test, Y_test, W, b)\n",
    "print('Test error: {}'.format(calc_error(X_test, Y_test, W, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T22:24:28.320795Z",
     "start_time": "2020-02-03T22:24:28.205224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training loss curve.\n",
    "plt.title('Training loss curve')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
